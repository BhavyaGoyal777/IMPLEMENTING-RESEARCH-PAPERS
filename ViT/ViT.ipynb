{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader , random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3,std=[0.5]*3)\n",
    "    ]\n",
    ")\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE,IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3,std=[0.5]*3)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [01:34<00:00, 1.80MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root='data', train=True, download=True, transform=train_transforms)\n",
    "val_dataset = datasets.CIFAR10(root='data', train=False, download=True, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##here the images are given in the shape B,channels,height,width\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self,num_channels = 3,\n",
    "                 patch_size = 16,\n",
    "                 embedding_dim = 768):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.patched_embeddings = nn.Conv2d(in_channels= num_channels,out_channels=embedding_dim,kernel_size=patch_size,stride=patch_size,padding=0)\n",
    "        self.flatten_embeddings= nn.Flatten(2,3)\n",
    "\n",
    "                                            \n",
    "    def forward(self,x):\n",
    "        image_resolution = x.shape[-1] #used to check the comaptability with the patch size \n",
    "        assert image_resolution % self.patch_size == 0\n",
    "\n",
    "        x_patched = self.patched_embeddings(x)\n",
    "        x_flatten = self.flatten_embeddings(x_patched) #shape->(batch,768,4)\n",
    "\n",
    "        return x_flatten.permute(0,2,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,num_heads = 12 ,\n",
    "                 embedding_dim = 768,\n",
    "                 attention_dropout = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.mha = nn.MultiheadAttention(embedding_dim,num_heads,attention_dropout,batch_first = True)\n",
    "    def forward(self,x):\n",
    "        x = self.layer_norm(x)\n",
    "        context_vec,_= self.mha(key = x, value = x , query = x,need_weights = False) # the need wieghts is the attention score which we dont want\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self,in_embedding_dim = 768,\n",
    "                 hidden_emb_size = 3072,\n",
    "                 dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(in_embedding_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_embedding_dim,hidden_emb_size,bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=hidden_emb_size,out_features=in_embedding_dim,bias=False),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, num_heads =12 , \n",
    "                 embedding_dim = 768,\n",
    "                 drop_out = 0.1,\n",
    "                 hidden_emb = 3072,\n",
    "                 attn_drop = 0\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha_layer = MultiHeadAttention(embedding_dim=embedding_dim,num_heads=num_heads,attention_dropout=drop_out)\n",
    "        self.ffn = FFN(in_embedding_dim=embedding_dim,hidden_emb_size=hidden_emb,dropout=drop_out)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.mha_layer(x) + x #residual connections\n",
    "        x = self.ffn(x) + x ##residual connections\n",
    "\n",
    "        return x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_size = 768,\n",
    "                 patch_size = 16,\n",
    "                 num_heads = 12,\n",
    "                 hidden_emb = 3072,\n",
    "                 dropout = 0.1,\n",
    "                 attn_dropout = 0,\n",
    "                 image_height = 224,\n",
    "                 image_Width = 224,\n",
    "                 num_channels = 3,\n",
    "                 classes = 1000,\n",
    "                 pos_drop = 0.1,\n",
    "                 num_block = 12\n",
    "\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.num_patches = image_height * image_Width // (patch_size * patch_size)\n",
    "        self.patch_embedding =  PatchEmbeddings(patch_size=patch_size,embedding_dim=embedding_size,num_channels=3)\n",
    "        self.positional_embedding = nn.Parameter(torch.rand(1,self.num_patches+1,embedding_size),requires_grad = True)\n",
    "        self.cls_token = nn.Parameter(torch.rand(1,1,embedding_size),requires_grad = True)\n",
    "        self.positional_drop_out = nn.Dropout(p = pos_drop)\n",
    "\n",
    "\n",
    "        self.encoder_block = nn.Sequential(\n",
    "            *[encoder_block(\n",
    "                num_heads=num_heads,\n",
    "                embedding_dim=embedding_size,\n",
    "                hidden_emb=hidden_emb,\n",
    "                drop_out=attn_dropout) for _ in range(num_block)]\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_size,\n",
    "                      out_features=classes)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embedding(x)\n",
    "        cls_token = self.cls_token.expand(batch_size,-1,-1)#okay so here we first reate the cls token embedding to match our batch size so that it can be appended to evry data in the batch\n",
    "        x = torch.cat((x,cls_token),dim=1)\n",
    "        x = x + self.positional_embedding\n",
    "\n",
    "        x = self.positional_drop_out(x)\n",
    "        x =self.encoder_block(x)\n",
    "        x = self.classifier(x[:,-1])\n",
    "\n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  ViT(\n",
    "    embedding_size=768,\n",
    "    patch_size=16,\n",
    "    num_block=12,\n",
    "    num_channels=3,\n",
    "    classes=10,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (patch_embedding): PatchEmbeddings(\n",
       "    (patched_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (flatten_embeddings): Flatten(start_dim=2, end_dim=3)\n",
       "  )\n",
       "  (positional_drop_out): Dropout(p=0.1, inplace=False)\n",
       "  (encoder_block): Sequential(\n",
       "    (0): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): encoder_block(\n",
       "      (mha_layer): MultiHeadAttention(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ffn): FFN(\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (4): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(),lr=4e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, val_loader, calc_loss, optimizer, device, epochs=10):\n",
    "    model.to(device)\n",
    "    best_val_acc = 0.0\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "\n",
    "        train_loss, train_correct, total = 0.0, 0, 0\n",
    "\n",
    "        # Training loop with tqdm\n",
    "        pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch+1}\")\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = calc_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Log batch-level metrics (optional)\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        train_acc = 100. * train_correct / total\n",
    "        train_loss /= total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "\n",
    "                loss = calc_loss(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_loss /= val_total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.2f}%\")\n",
    "\n",
    "       \n",
    "\n",
    "        # Save best model\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")\n",
    "        print(f\"Model saved at epoch {epoch+1}\")\n",
    "\n",
    "    print(f\"\\nTraining complete. Best Val Acc: {best_val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, val_loader, loss, optimizer, device, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
