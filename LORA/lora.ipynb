{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "okay so here is the explanation which I understood\n",
    "as we fintune the transformer block we instead of updation the\n",
    "whole paramters we update some percentage of full paramters \n",
    "lets say our layer has weight matrix dimension (d_in,d_out)\n",
    "now what happens is the authors of the paper found that \n",
    "generally in the weight matrix there are vectors which are not dependent \n",
    "therefore they are extra paramters which are not necessary to train so \n",
    "we can apply matrix decompostion where W =  A x B\n",
    "where A = d_in X R and B = R X d_out therefore we add the matrix after training\n",
    "\"\"\"\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self,in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim,rank))\n",
    "        nn.init.kaiming_uniform(self.A, a=math.sqrt(5))\n",
    "\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "    def forward(self, x):\n",
    "        # print(\"A shape---->\",self.A.shape)\n",
    "        # print(\"B shape---->\",self.B.shape)\n",
    "        # print(x.shape)\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self,linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(linear.in_features,linear.out_features, rank,alpha)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model,rank,alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module,torch.nn.Linear):\n",
    "            setattr(model, name, LinearWithLoRA(module,rank,alpha))\n",
    "        else:\n",
    "            replace_linear_with_lora(module, rank, alpha)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lets say we take an example of gpt-2-small(124M)\n",
    "if we replace just all the linear layers with Lora \n",
    "linear layers \n",
    "we have linear layers in attention head\n",
    "feed forward and linear ouptut layer \n",
    "\n",
    "multi head attention we have key value and quey matrix \n",
    "where for each matrix we have (d_in,d_out) so 3 x (d_in,d_out)\n",
    "okay then the projection layer which is (d_out,d_out)\n",
    "\n",
    "then we have ffn where we have 2 layers with relu activation\n",
    "the ffn1 = (emb_dim,4 * emb_dim)\n",
    "    ffn2 = (4 * emb_dim,emb_dim)\n",
    "\n",
    "    and the last linear output layer (emb_dim,vocab_size)\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12}\n",
    "\n",
    "(3 * 768 * 768 + 768 * 768 + 768 * 4 * 768 * 2) * 12 + (768 * 50257) = 123,532,032\n",
    "now we take r as 2 suppose \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
