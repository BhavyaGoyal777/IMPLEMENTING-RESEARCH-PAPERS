{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd665a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets wandb trl\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3053114-35b6-4038-b66f-e0fb3c298e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a093403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "!wandb login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caa2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project = \"ORPO\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bdb0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from  trl import setup_chat_format\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cbd2b91-0531-47ca-b938-3772be28e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "676d2743-4896-4798-b9c1-168006a75e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flash_attention_2'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20b0411e-924d-4ccf-b4d7-82cc5ee5bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORPO:\n",
    "  def __init__(self, model, device, tokenizer):\n",
    "\n",
    "\n",
    "    self.model = model\n",
    "    self.device=device\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "  def ORloss(self, datapoint):\n",
    "\n",
    "\n",
    "\n",
    "    self.win_prompt = datapoint['chosen']\n",
    "    self.lose_prompt = datapoint['rejected']\n",
    "\n",
    "   \n",
    "    self.chosen_log_probs = torch.nn.functional.log_softmax(self.model(**self.win_prompt).logits, dim=-1)\n",
    "   \n",
    "    self.chosen_log_probs = torch.gather(self.chosen_log_probs, -1, self.win_prompt['input_ids'].unsqueeze(-1)).squeeze(-1) \n",
    "    \n",
    "    self.chosen_log_probs = self.chosen_log_probs * (self.win_prompt['attention_mask'])\n",
    "    \n",
    "    self.chosen_log_probs = self.chosen_log_probs.sum(dim=-1)\n",
    "    \n",
    "\n",
    "    self.rejected_log_probs = torch.nn.functional.log_softmax(self.model(**self.lose_prompt).logits, dim=-1)\n",
    "    self.rejected_log_probs = torch.gather(self.rejected_log_probs, -1, self.lose_prompt['input_ids'].unsqueeze(-1)).squeeze(-1)\n",
    "    self.rejected_log_probs = self.rejected_log_probs * (self.lose_prompt['attention_mask'])\n",
    "    self.rejected_log_probs = self.rejected_log_probs.sum(dim=-1)\n",
    "    \n",
    "      \n",
    "    self.log_odds1 = torch.log1p(torch.exp(self.chosen_log_probs)) - (1 - torch.log1p(torch.exp(self.chosen_log_probs)))\n",
    "    self.log_odds2 = torch.log1p(torch.exp(self.rejected_log_probs)) - (1 - torch.log1p(torch.exp(self.rejected_log_probs))) \n",
    "    \n",
    "\n",
    "     \n",
    "    self.OR = -nn.functional.logsigmoid(self.log_odds1 - self.log_odds2).mean()\n",
    "\n",
    "    return self.OR  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "195d52a2-39a0-45b3-b146-3c65f9baaf8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig  \n",
    "import torch\n",
    "\n",
    "\n",
    "base_model = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "new_model = \"Orpo-SMALLM-v2-135M\"\n",
    "\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\")\n",
    "\n",
    "\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def prepare_model_for_kbit_training(model):\n",
    " \n",
    "    return model\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b28d445a-aa9b-4943-b868-c3ab633d4682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = \"argilla/distilabel-math-preference-dpo\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0925d7c2-2d8c-4601-81bb-402a72eb93ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['metadata', 'instruction', 'chosen_response', 'chosen_rating', 'rejected_response', 'rejected_rating'],\n",
      "    num_rows: 990\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65c69081-078b-4dc7-8aa6-6dcd7a83d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orpo_collate_fn_merged_prompt(batch):\n",
    "\n",
    "    merged_chosen_prompts = []\n",
    "    merged_rejected_prompts = []\n",
    "\n",
    "    for sample in batch:\n",
    "\n",
    "        \n",
    "        prompt = sample['instruction']\n",
    "        chosen_data = sample['chosen_response']\n",
    "        chosen_data = \"Instruction: \" + prompt + \"\\n\" + \"Output: \" + chosen_data + \"\\n\"\n",
    "      \n",
    "        rejected_data = sample['rejected_response']\n",
    "        rejected_data =  \"Instruction: \" + prompt + \"\\n\" + \"Output: \" + rejected_data + \"\\n\"\n",
    "\n",
    "\n",
    "        merged_chosen_prompts.append(chosen_data)\n",
    "\n",
    "\n",
    "        merged_rejected_prompts.append(rejected_data)\n",
    "\n",
    "    tokenized_win_prompt = tokenizer(merged_chosen_prompts, max_length = 1024, padding='max_length', truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    tokenized_lose_prompt = tokenizer(merged_rejected_prompts, max_length = 1024, truncation=True, padding='max_length', return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "       \n",
    "        'chosen': tokenized_win_prompt, \n",
    "        'rejected': tokenized_lose_prompt \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1c290a8-c69d-4bcf-9c14-b1fc3ef12410",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=orpo_collate_fn_merged_prompt)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=orpo_collate_fn_merged_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "552bc8e0-5e78-4265-9cfe-d5f7c7a1e796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chosen': {'input_ids': tensor([[25464,    42,  1073,  ...,     2,     2,     2],\n",
      "        [25464,    42, 16222,  ...,     2,     2,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}, 'rejected': {'input_ids': tensor([[25464,    42,  1073,  ...,     2,     2,     2],\n",
      "        [25464,    42, 16222,  ...,     2,     2,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23f79a27-c42f-4eed-bda9-d8d4cdd6ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "beta = 0.2\n",
    "max_lr = 8e-6\n",
    "betas = (0.95, 0.99)\n",
    "weight_decay=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c5fa488-8581-44b6-a6fa-41454e8b352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c2ada690-5f08-414e-a467-208a80c9da86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 460,800 || all params: 134,975,808 || trainable%: 0.3414\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 2. Freeze the base model's parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze the parameter\n",
    "    \n",
    "\n",
    "# 3. Configure PEFT (e.g., LoRA)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,  # Rank of the LoRA matrices\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" \n",
    ")\n",
    "\n",
    "# 4. Apply PEFT\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # Verify trainable parameters.  Should be PEFT params only\n",
    "\n",
    "r\n",
    "\n",
    "# If you want to use bitsandbytes Adam8bit, then\n",
    "import bitsandbytes as bnb\n",
    "optimizer = bnb.optim.Adam8bit(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d16903e6-246a-44ee-8685-30964dfbe553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/990 [00:14<11:43,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20: val loss 1.4689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 40/990 [00:36<12:13,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 40: val loss 1.4603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 60/990 [00:58<11:20,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 60: val loss 1.4699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 80/990 [01:21<11:06,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 80: val loss 1.4698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/990 [01:43<11:06,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: val loss 1.4646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 120/990 [02:05<11:39,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 120: val loss 1.4723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 140/990 [02:27<10:20,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 140: val loss 1.4567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 160/990 [02:50<10:11,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 160: val loss 1.4689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 180/990 [03:12<09:51,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 180: val loss 1.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200/990 [03:34<09:50,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: val loss 1.4486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 220/990 [03:56<09:26,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 220: val loss 1.4496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 240/990 [04:18<09:17,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 240: val loss 1.4552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 260/990 [04:41<08:53,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 260: val loss 1.4635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 280/990 [05:03<08:55,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 280: val loss 1.4469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 300/990 [05:25<08:20,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300: val loss 1.4433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 320/990 [05:47<08:13,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 320: val loss 1.4607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 340/990 [06:09<08:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 340: val loss 1.4554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 360/990 [06:31<08:16,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 360: val loss 1.4646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 380/990 [06:53<07:24,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 380: val loss 1.4649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400/990 [07:16<07:10,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: val loss 1.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 420/990 [07:38<06:52,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 420: val loss 1.4374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 440/990 [08:00<06:48,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 440: val loss 1.4404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 460/990 [08:23<06:54,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 460: val loss 1.4420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 480/990 [08:45<06:13,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 480: val loss 1.4338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 500/990 [09:07<05:58,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: val loss 1.4288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 520/990 [09:30<05:48,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 520: val loss 1.4497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 540/990 [09:52<05:43,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 540: val loss 1.4287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 560/990 [10:14<05:13,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 560: val loss 1.4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 580/990 [10:36<05:02,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 580: val loss 1.4369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 600/990 [10:58<04:47,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600: val loss 1.4460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 620/990 [11:20<04:28,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 620: val loss 1.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 640/990 [11:43<04:23,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 640: val loss 1.4458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 660/990 [12:05<04:04,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 660: val loss 1.4453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 680/990 [12:27<03:47,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 680: val loss 1.4460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 700/990 [12:49<03:34,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 700: val loss 1.4443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 720/990 [13:11<03:17,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 720: val loss 1.4281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 740/990 [13:34<03:07,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 740: val loss 1.4294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 760/990 [13:56<02:49,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 760: val loss 1.4421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 780/990 [14:18<02:32,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 780: val loss 1.4379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 800/990 [14:40<02:17,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800: val loss 1.4362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 820/990 [15:02<02:07,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 820: val loss 1.4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 840/990 [15:25<01:52,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 840: val loss 1.4172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 860/990 [15:47<01:35,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 860: val loss 1.4346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 880/990 [16:09<01:21,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 880: val loss 1.4305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 900/990 [16:31<01:10,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 900: val loss 1.4117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 920/990 [16:54<00:51,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 920: val loss 1.4349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 940/990 [17:16<00:36,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 940: val loss 1.4304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 960/990 [17:38<00:22,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 960: val loss 1.4572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 980/990 [18:00<00:07,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 980: val loss 1.4308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 989/990 [18:14<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 989: val loss 1.4153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [18:22<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # Import tqdm if you haven't already\n",
    "\n",
    "\n",
    "\n",
    "# Add batch_size here\n",
    "total_steps = 2 * len(train_loader)\n",
    "batch_size = train_loader.batch_size  # Access batch size from the train_loader\n",
    "\n",
    "# Assuming val_loader and train_loader are already defined\n",
    "model.train()\n",
    "\n",
    "val_iterator = iter(val_loader)\n",
    "train_iterator = iter(train_loader)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def estimate_loss(batch_size): \n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    # Create a new validation loader iterator each time\n",
    "    temp_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                 collate_fn=orpo_collate_fn_merged_prompt)\n",
    "    temp_val_iterator = iter(temp_val_loader)\n",
    "\n",
    "    for split in ['val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "           \n",
    "            try:\n",
    "                text = next(temp_val_iterator)\n",
    "            except StopIteration:\n",
    "                \n",
    "                temp_val_iterator = iter(temp_val_loader)\n",
    "                text = next(temp_val_iterator)\n",
    "\n",
    "            targets = text['chosen']['input_ids']\n",
    "            logits = model(**text['chosen']).logits\n",
    "            logits = logits[..., :-1, :].contiguous()\n",
    "            targets = targets[..., 1:].contiguous()\n",
    "\n",
    "            batch_size, block_size, embeddings_dims = logits.shape\n",
    "            logits = logits.view(batch_size * block_size, embeddings_dims)\n",
    "            targets = targets.view(batch_size * block_size)\n",
    "\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets,\n",
    "                                                       ignore_index=tokenizer.pad_token_id) + beta * orpo.ORloss(\n",
    "                text)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "for step in tqdm(range(total_steps)):\n",
    "    if (step % eval_iters == 0 and step != 0) or step == total_steps - 1:\n",
    "        losses = estimate_loss(batch_size)  # Pass batch_size here\n",
    "        print(f\"step {step}: val loss {losses['val']:.4f}\")\n",
    "       \n",
    "        wandb.log({\n",
    "            \"step\": step,\n",
    "            \"val_loss\": losses['val']\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        text = next(train_iterator)\n",
    "    except StopIteration:\n",
    "        train_iterator = iter(train_loader)\n",
    "        text = next(train_iterator)\n",
    "\n",
    "    targets = text['chosen']['input_ids']\n",
    "    logits = model(**text['chosen']).logits\n",
    "    targets = targets[..., 1:].contiguous()\n",
    "    logits = logits[..., :-1, :].contiguous()\n",
    "\n",
    "    batch_size, block_size, vocab_size = logits.shape\n",
    "    logits = logits.view(batch_size * block_size, vocab_size)\n",
    "    targets = targets.view(batch_size * block_size)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, targets,\n",
    "                                               ignore_index=tokenizer.pad_token_id) + beta * orpo.ORloss(text)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    wandb.log({\n",
    "        \"step\": step,\n",
    "        \"training_loss\": loss.item()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de380437-fb0d-4d4f-adc7-9262f4c4b201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
