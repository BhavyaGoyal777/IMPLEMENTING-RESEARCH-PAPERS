{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self,n_embd,dropout):\n",
    "        super().__init__()\n",
    "        self.net = (\n",
    "            nn.Linear(n_embd,4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd,n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embed = 4\n",
    "batch = 2\n",
    "num_tokens = 3\n",
    "mh_output = torch.randn(batch,num_tokens,n_embed)\n",
    "\n",
    "topkgate_linear = nn.Linear(n_embed,num_experts)\n",
    "\n",
    "logits = topkgate_linear(mh_output)\n",
    "\n",
    "top_k_logits,top_k_indices = logits.topk(top_k,dim = -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0014, -0.0103],\n",
       "          [ 0.7816,  0.2050],\n",
       "          [ 1.2231,  0.9853]],\n",
       " \n",
       "         [[ 0.9302,  0.9230],\n",
       "          [ 0.3702, -0.0118],\n",
       "          [ 0.5589,  0.3120]]], grad_fn=<TopkBackward0>),\n",
       " tensor([[[3, 2],\n",
       "          [2, 1],\n",
       "          [2, 3]],\n",
       " \n",
       "         [[1, 2],\n",
       "          [2, 1],\n",
       "          [1, 2]]]),\n",
       " tensor([[[-6.0454e-02, -2.4748e-01, -1.0280e-02,  1.3634e-03],\n",
       "          [-8.4506e-02,  2.0497e-01,  7.8162e-01, -5.4297e-01],\n",
       "          [ 4.1327e-01, -2.2426e+00,  1.2231e+00,  9.8526e-01]],\n",
       " \n",
       "         [[-2.9083e-01,  9.3017e-01,  9.2298e-01, -9.4899e-01],\n",
       "          [-3.3395e-01, -1.1830e-02,  3.7025e-01, -4.5657e-01],\n",
       "          [-5.0865e-01,  5.5890e-01,  3.1198e-01, -1.4030e-03]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_logits , top_k_indices , logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.full_like(logits , float(\"-inf\"))\n",
    "sparse_logits = zeros.scatter(-1,top_k_indices,top_k_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   -inf,    -inf, -0.0103,  0.0014],\n",
       "         [   -inf,  0.2050,  0.7816,    -inf],\n",
       "         [   -inf,    -inf,  1.2231,  0.9853]],\n",
       "\n",
       "        [[   -inf,  0.9302,  0.9230,    -inf],\n",
       "         [   -inf, -0.0118,  0.3702,    -inf],\n",
       "         [   -inf,  0.5589,  0.3120,    -inf]]], grad_fn=<ScatterBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating_outputs = F.softmax(sparse_logits,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.4971, 0.5029],\n",
       "         [0.0000, 0.3597, 0.6403, 0.0000],\n",
       "         [0.0000, 0.0000, 0.5592, 0.4408]],\n",
       "\n",
       "        [[0.0000, 0.5018, 0.4982, 0.0000],\n",
       "         [0.0000, 0.4056, 0.5944, 0.0000],\n",
       "         [0.0000, 0.5614, 0.4386, 0.0000]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKROuter(nn.Module):\n",
    "    def __init__(self,emb_dim, num_experts, top_k):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.linear = nn.Linear(n_embed,num_experts)\n",
    "\n",
    "    def forward(self,mh_output):\n",
    "        logits = self.linear(mh_output)\n",
    "        top_k_logits,top_k_indices = logits.topk(top_k,dim=-1)\n",
    "        zeros = torch.full_like(logits,float(\"-inf\"))\n",
    "\n",
    "        sparse_logits = zeros.scatter(-1,top_k_indices,top_k_logits)\n",
    "\n",
    "        router_output = F.softmax(sparse_logits,dim=-1)\n",
    "\n",
    "        return router_output,top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embed = 32\n",
    "\n",
    "\n",
    "mh_output = torch.randn(3,5,n_embed)\n",
    "top_k_gate = TopKROuter(n_embed,num_experts=num_experts,top_k=top_k)\n",
    "output,indices = top_k_gate(mh_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyTopkRouter(nn.Module):\n",
    "    def __init__(self,n_embed,num_experts ,top_k):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        self.topkroute_linear  = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear  = nn.Linear(n_embed,num_experts)\n",
    "    def forward(self,mh_output):\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "        gaussian_noise = torch.randn_like(logits)\n",
    "\n",
    "        final_noise = F.softplus(noise_logits) * gaussian_noise\n",
    "        final_logits = final_noise + logits\n",
    "\n",
    "        final_logits,indices = final_logits.topk(self.top_k,dim=-1)\n",
    "        zeros = torch.full_like(logits,float(\"-inf\"))\n",
    "\n",
    "        sparse_logits = zeros.scatter(-1,indices,final_logits)\n",
    "\n",
    "        router_output = F.softmax(sparse_logits,dim=-1)\n",
    "\n",
    "        return router_output,indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 8]),\n",
       " tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.2990, 0.0000, 0.7010, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.6048, 0.3952, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5774, 0.4226, 0.0000],\n",
       "          [0.0000, 0.4575, 0.0000, 0.5425, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.4817, 0.0000, 0.5183, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3991, 0.6009],\n",
       "          [0.0000, 0.0000, 0.4223, 0.5777, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.4879, 0.0000, 0.5121, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " torch.Size([2, 4, 2]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_experts = 8\n",
    "top_k = 2 \n",
    "n_embed = 16\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embed)\n",
    "noisy_top_k_gate = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "gating_output, indices = noisy_top_k_gate(mh_output)\n",
    "gating_output.shape,gating_output,indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input x:\", x.shape)\n",
    "        gating_output, indices = self.router(x)\n",
    "        print(\"gating_output:\", gating_output.shape, \"\\n\", gating_output)\n",
    "        print(\"indices:\", indices.shape, \"\\n\", indices)\n",
    "        \n",
    "        final_output = torch.zeros_like(x)\n",
    "        print(\"final_output:\", final_output.shape, \"\\n\", final_output)\n",
    "        \n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        print(\"flat_x:\", flat_x.shape, \"\\n\", flat_x)\n",
    "        \n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "        print(\"flat_gating_output:\", flat_gating_output.shape, \"\\n\", flat_gating_output)\n",
    "        \n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            print(f\"expert_mask for expert {i}:\", expert_mask.shape, \"\\n\", expert_mask)\n",
    "            \n",
    "            flat_mask = expert_mask.view(-1)\n",
    "            print(f\"flat_mask for expert {i}:\", flat_mask.shape, \"\\n\", flat_mask)\n",
    "            \n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                print(f\"expert_input for expert {i}:\", expert_input.shape, \"\\n\", expert_input)\n",
    "                \n",
    "                expert_output = expert(expert_input)\n",
    "                print(f\"expert_output for expert {i}:\", expert_output.shape, \"\\n\", expert_output)\n",
    "                \n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                print(f\"gating_scores for expert {i}:\", gating_scores.shape, \"\\n\", gating_scores)\n",
    "                \n",
    "                weighted_output = expert_output * gating_scores\n",
    "                print(f\"weighted_output for expert {i}:\", weighted_output.shape, \"\\n\", weighted_output)\n",
    "                \n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "        \n",
    "        print(\"final_output after processing:\", final_output.shape, \"\\n\", final_output)\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x: torch.Size([4, 8, 6])\n",
      "gating_output: torch.Size([4, 8, 4]) \n",
      " tensor([[[0.0000, 0.2498, 0.7502, 0.0000],\n",
      "         [0.7749, 0.0000, 0.2251, 0.0000],\n",
      "         [0.0000, 0.0000, 0.3653, 0.6347],\n",
      "         [0.4675, 0.0000, 0.0000, 0.5325],\n",
      "         [0.0000, 0.3782, 0.6218, 0.0000],\n",
      "         [0.0000, 0.6314, 0.3686, 0.0000],\n",
      "         [0.6414, 0.3586, 0.0000, 0.0000],\n",
      "         [0.0000, 0.8138, 0.1862, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.3821, 0.6179, 0.0000],\n",
      "         [0.6282, 0.0000, 0.3718, 0.0000],\n",
      "         [0.0000, 0.5740, 0.0000, 0.4260],\n",
      "         [0.0000, 0.0000, 0.4473, 0.5527],\n",
      "         [0.4817, 0.0000, 0.5183, 0.0000],\n",
      "         [0.0000, 0.0000, 0.2734, 0.7266],\n",
      "         [0.4092, 0.0000, 0.5908, 0.0000],\n",
      "         [0.0000, 0.3890, 0.0000, 0.6110]],\n",
      "\n",
      "        [[0.0000, 0.7543, 0.2457, 0.0000],\n",
      "         [0.0000, 0.0000, 0.1928, 0.8072],\n",
      "         [0.9380, 0.0000, 0.0000, 0.0620],\n",
      "         [0.0000, 0.3392, 0.6608, 0.0000],\n",
      "         [0.7222, 0.0000, 0.2778, 0.0000],\n",
      "         [0.4829, 0.5171, 0.0000, 0.0000],\n",
      "         [0.6105, 0.0000, 0.3895, 0.0000],\n",
      "         [0.0000, 0.3151, 0.6849, 0.0000]],\n",
      "\n",
      "        [[0.1846, 0.0000, 0.8154, 0.0000],\n",
      "         [0.5484, 0.4516, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.6195, 0.3805],\n",
      "         [0.4195, 0.0000, 0.5805, 0.0000],\n",
      "         [0.0000, 0.0000, 0.6423, 0.3577],\n",
      "         [0.0000, 0.3511, 0.6489, 0.0000],\n",
      "         [0.0000, 0.1599, 0.8401, 0.0000],\n",
      "         [0.3874, 0.0000, 0.0000, 0.6126]]], grad_fn=<SoftmaxBackward0>)\n",
      "indices: torch.Size([4, 8, 2]) \n",
      " tensor([[[2, 1],\n",
      "         [0, 2],\n",
      "         [3, 2],\n",
      "         [3, 0],\n",
      "         [2, 1],\n",
      "         [1, 2],\n",
      "         [0, 1],\n",
      "         [1, 2]],\n",
      "\n",
      "        [[2, 1],\n",
      "         [0, 2],\n",
      "         [1, 3],\n",
      "         [3, 2],\n",
      "         [2, 0],\n",
      "         [3, 2],\n",
      "         [2, 0],\n",
      "         [3, 1]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [3, 2],\n",
      "         [0, 3],\n",
      "         [2, 1],\n",
      "         [0, 2],\n",
      "         [1, 0],\n",
      "         [0, 2],\n",
      "         [2, 1]],\n",
      "\n",
      "        [[2, 0],\n",
      "         [0, 1],\n",
      "         [2, 3],\n",
      "         [2, 0],\n",
      "         [2, 3],\n",
      "         [2, 1],\n",
      "         [2, 1],\n",
      "         [3, 0]]])\n",
      "final_output: torch.Size([4, 8, 6]) \n",
      " tensor([[[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]]])\n",
      "flat_x: torch.Size([32, 6]) \n",
      " tensor([[ 1.2299e+00,  5.2441e-01,  1.4837e+00,  1.5396e-01, -2.1697e-01,\n",
      "         -8.8170e-01],\n",
      "        [-7.1175e-01, -3.1869e-01,  4.7824e-02, -5.2963e-01, -1.4327e+00,\n",
      "          2.3175e-01],\n",
      "        [-2.8373e-01,  1.0245e+00,  9.2586e-01,  1.9426e+00, -3.8501e-01,\n",
      "          6.0114e-01],\n",
      "        [ 6.1884e-01,  2.2128e+00, -9.4502e-01,  3.9714e-01, -1.3050e+00,\n",
      "          4.1430e-01],\n",
      "        [-1.3578e+00, -3.6746e-01,  1.3089e+00, -3.0863e-01, -1.5820e-01,\n",
      "          1.4662e+00],\n",
      "        [ 6.8992e-01,  1.6619e+00,  4.2371e-01, -1.2295e+00, -7.1474e-01,\n",
      "          3.0786e-01],\n",
      "        [-1.2869e+00,  9.8815e-01,  8.1004e-01, -5.5542e-01,  1.6155e+00,\n",
      "          2.3379e-01],\n",
      "        [ 4.9741e-01,  1.1011e+00,  2.1932e-01, -1.8050e+00,  7.0224e-01,\n",
      "          5.9832e-01],\n",
      "        [-5.9661e-01,  1.4202e+00, -7.0488e-01,  2.6981e-01, -3.5171e-01,\n",
      "         -8.2836e-01],\n",
      "        [-6.4758e-01, -8.0608e-01, -2.0871e-01,  2.6211e-01, -4.5044e-01,\n",
      "         -3.9843e-01],\n",
      "        [ 1.2632e+00,  5.8370e-01, -5.3027e-01,  2.6402e-01,  1.3398e+00,\n",
      "          6.9470e-01],\n",
      "        [ 7.6762e-01,  9.4395e-03,  3.9564e-01,  3.2025e-01, -2.8774e-01,\n",
      "          2.5670e-01],\n",
      "        [-6.7316e-02, -4.8072e-01, -2.0145e-03, -2.9684e-01,  4.9095e-01,\n",
      "         -3.7736e-01],\n",
      "        [ 1.0783e+00, -1.2894e+00,  2.5983e-01,  1.1599e+00,  8.2941e-01,\n",
      "          1.7601e-02],\n",
      "        [-1.1930e+00, -7.3533e-01, -5.9583e-01, -6.1060e-01, -1.4336e-01,\n",
      "          4.0583e-01],\n",
      "        [ 7.2765e-01, -2.2897e-01, -3.8732e-01,  1.1962e+00,  1.0144e+00,\n",
      "         -1.3589e+00],\n",
      "        [-7.2012e-01,  4.4555e-01,  1.4524e-01, -2.8537e-01, -7.5935e-01,\n",
      "          1.0620e-01],\n",
      "        [-7.1099e-01,  3.5681e-01,  6.9835e-01,  2.3638e+00,  5.8684e-01,\n",
      "          1.2924e+00],\n",
      "        [-1.4764e+00, -3.6916e-01,  1.3290e+00,  1.5123e-01,  3.0084e-01,\n",
      "          3.9178e-01],\n",
      "        [ 4.9582e-01, -1.6009e+00,  2.9994e-01,  1.2335e+00, -2.5198e+00,\n",
      "          1.0030e+00],\n",
      "        [-5.5582e-01, -1.7669e+00, -5.2814e-01, -1.2567e+00, -1.8438e-01,\n",
      "         -3.8586e-02],\n",
      "        [-8.7689e-02, -2.3135e+00, -5.4886e-01, -4.3784e-01, -1.4740e+00,\n",
      "          1.4664e+00],\n",
      "        [-2.5843e+00, -4.8744e-01,  7.8507e-03,  3.6422e-01, -7.7910e-01,\n",
      "          3.4041e-01],\n",
      "        [ 5.0866e-01,  1.8129e-01,  7.0565e-03,  5.3748e-01,  1.7128e+00,\n",
      "         -1.0169e+00],\n",
      "        [ 1.0021e+00,  5.1247e-01,  1.6864e+00, -6.9051e-01,  5.6580e-01,\n",
      "          1.4997e-01],\n",
      "        [ 9.6602e-02, -1.8567e-01, -1.6892e-01, -9.0742e-01, -6.4971e-01,\n",
      "          1.7555e+00],\n",
      "        [ 7.1586e-01, -3.8600e-01,  7.5606e-01,  4.5368e-01,  5.5452e-01,\n",
      "          6.4531e-01],\n",
      "        [-1.2511e+00, -6.2662e-01, -1.6361e-01,  5.7103e-01, -2.5860e+00,\n",
      "          1.1485e+00],\n",
      "        [ 6.8176e-01,  1.6960e+00,  2.9258e-01, -1.0163e+00,  7.7617e-01,\n",
      "         -1.7823e+00],\n",
      "        [ 1.3038e+00, -6.6918e-01,  4.7511e-01, -4.2731e-02, -9.3523e-01,\n",
      "          3.3459e-01],\n",
      "        [ 8.4746e-01, -1.3469e-01, -9.6825e-01, -1.6712e+00, -6.4324e-01,\n",
      "         -1.3491e+00],\n",
      "        [-1.9858e-02,  1.0562e+00,  1.9374e+00,  9.2023e-01,  5.0427e-01,\n",
      "          1.5896e+00]])\n",
      "flat_gating_output: torch.Size([32, 4]) \n",
      " tensor([[0.0000, 0.2498, 0.7502, 0.0000],\n",
      "        [0.7749, 0.0000, 0.2251, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3653, 0.6347],\n",
      "        [0.4675, 0.0000, 0.0000, 0.5325],\n",
      "        [0.0000, 0.3782, 0.6218, 0.0000],\n",
      "        [0.0000, 0.6314, 0.3686, 0.0000],\n",
      "        [0.6414, 0.3586, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8138, 0.1862, 0.0000],\n",
      "        [0.0000, 0.3821, 0.6179, 0.0000],\n",
      "        [0.6282, 0.0000, 0.3718, 0.0000],\n",
      "        [0.0000, 0.5740, 0.0000, 0.4260],\n",
      "        [0.0000, 0.0000, 0.4473, 0.5527],\n",
      "        [0.4817, 0.0000, 0.5183, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2734, 0.7266],\n",
      "        [0.4092, 0.0000, 0.5908, 0.0000],\n",
      "        [0.0000, 0.3890, 0.0000, 0.6110],\n",
      "        [0.0000, 0.7543, 0.2457, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1928, 0.8072],\n",
      "        [0.9380, 0.0000, 0.0000, 0.0620],\n",
      "        [0.0000, 0.3392, 0.6608, 0.0000],\n",
      "        [0.7222, 0.0000, 0.2778, 0.0000],\n",
      "        [0.4829, 0.5171, 0.0000, 0.0000],\n",
      "        [0.6105, 0.0000, 0.3895, 0.0000],\n",
      "        [0.0000, 0.3151, 0.6849, 0.0000],\n",
      "        [0.1846, 0.0000, 0.8154, 0.0000],\n",
      "        [0.5484, 0.4516, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6195, 0.3805],\n",
      "        [0.4195, 0.0000, 0.5805, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6423, 0.3577],\n",
      "        [0.0000, 0.3511, 0.6489, 0.0000],\n",
      "        [0.0000, 0.1599, 0.8401, 0.0000],\n",
      "        [0.3874, 0.0000, 0.0000, 0.6126]], grad_fn=<ViewBackward0>)\n",
      "expert_mask for expert 0: torch.Size([4, 8]) \n",
      " tensor([[False,  True, False,  True, False, False,  True, False],\n",
      "        [False,  True, False, False,  True, False,  True, False],\n",
      "        [False, False,  True, False,  True,  True,  True, False],\n",
      "        [ True,  True, False,  True, False, False, False,  True]])\n",
      "flat_mask for expert 0: torch.Size([32]) \n",
      " tensor([False,  True, False,  True, False, False,  True, False, False,  True,\n",
      "        False, False,  True, False,  True, False, False, False,  True, False,\n",
      "         True,  True,  True, False,  True,  True, False,  True, False, False,\n",
      "        False,  True])\n",
      "expert_input for expert 0: torch.Size([14, 6]) \n",
      " tensor([[-7.1175e-01, -3.1869e-01,  4.7824e-02, -5.2963e-01, -1.4327e+00,\n",
      "          2.3175e-01],\n",
      "        [ 6.1884e-01,  2.2128e+00, -9.4502e-01,  3.9714e-01, -1.3050e+00,\n",
      "          4.1430e-01],\n",
      "        [-1.2869e+00,  9.8815e-01,  8.1004e-01, -5.5542e-01,  1.6155e+00,\n",
      "          2.3379e-01],\n",
      "        [-6.4758e-01, -8.0608e-01, -2.0871e-01,  2.6211e-01, -4.5044e-01,\n",
      "         -3.9843e-01],\n",
      "        [-6.7316e-02, -4.8072e-01, -2.0145e-03, -2.9684e-01,  4.9095e-01,\n",
      "         -3.7736e-01],\n",
      "        [-1.1930e+00, -7.3533e-01, -5.9583e-01, -6.1060e-01, -1.4336e-01,\n",
      "          4.0583e-01],\n",
      "        [-1.4764e+00, -3.6916e-01,  1.3290e+00,  1.5123e-01,  3.0084e-01,\n",
      "          3.9178e-01],\n",
      "        [-5.5582e-01, -1.7669e+00, -5.2814e-01, -1.2567e+00, -1.8438e-01,\n",
      "         -3.8586e-02],\n",
      "        [-8.7689e-02, -2.3135e+00, -5.4886e-01, -4.3784e-01, -1.4740e+00,\n",
      "          1.4664e+00],\n",
      "        [-2.5843e+00, -4.8744e-01,  7.8507e-03,  3.6422e-01, -7.7910e-01,\n",
      "          3.4041e-01],\n",
      "        [ 1.0021e+00,  5.1247e-01,  1.6864e+00, -6.9051e-01,  5.6580e-01,\n",
      "          1.4997e-01],\n",
      "        [ 9.6602e-02, -1.8567e-01, -1.6892e-01, -9.0742e-01, -6.4971e-01,\n",
      "          1.7555e+00],\n",
      "        [-1.2511e+00, -6.2662e-01, -1.6361e-01,  5.7103e-01, -2.5860e+00,\n",
      "          1.1485e+00],\n",
      "        [-1.9858e-02,  1.0562e+00,  1.9374e+00,  9.2023e-01,  5.0427e-01,\n",
      "          1.5896e+00]])\n",
      "expert_output for expert 0: torch.Size([14, 6]) \n",
      " tensor([[-0.5271, -0.0863, -0.4702, -0.9762,  1.0042, -0.4344],\n",
      "        [ 0.0111, -0.4230, -0.7820, -0.5365,  0.3253,  0.2292],\n",
      "        [ 0.4680, -0.6812,  0.5139,  0.4200,  0.2055,  1.0983],\n",
      "        [-0.7923, -0.0894,  0.1308, -0.8174,  0.4559,  0.2479],\n",
      "        [-0.5030, -0.5732,  0.1730, -0.1697,  0.1185,  0.3863],\n",
      "        [-0.1534, -0.6207,  0.1001, -0.9213,  0.9349, -0.1962],\n",
      "        [-0.2600,  0.2596,  0.4658, -0.3737,  0.6774,  0.5846],\n",
      "        [-0.5818, -0.8275, -0.0471, -0.8275,  0.8318, -0.6308],\n",
      "        [-0.6419, -0.1777, -0.2800, -1.7099,  1.5350, -1.5185],\n",
      "        [-0.2906,  0.2771,  0.3373, -1.3554,  1.2304,  0.2865],\n",
      "        [-0.2835, -0.3259, -0.2095,  0.6958, -0.0773,  0.3773],\n",
      "        [ 0.2163, -0.6280, -0.4407, -0.8015,  1.2053, -0.9468],\n",
      "        [-0.6063,  0.6572, -0.4491, -1.9499,  1.6464, -0.7846],\n",
      "        [ 0.2175,  0.4757,  0.2970,  0.1162,  0.4137,  0.7358]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "gating_scores for expert 0: torch.Size([14, 1]) \n",
      " tensor([[0.7749],\n",
      "        [0.4675],\n",
      "        [0.6414],\n",
      "        [0.6282],\n",
      "        [0.4817],\n",
      "        [0.4092],\n",
      "        [0.9380],\n",
      "        [0.7222],\n",
      "        [0.4829],\n",
      "        [0.6105],\n",
      "        [0.1846],\n",
      "        [0.5484],\n",
      "        [0.4195],\n",
      "        [0.3874]], grad_fn=<UnsqueezeBackward0>)\n",
      "weighted_output for expert 0: torch.Size([14, 6]) \n",
      " tensor([[-0.4084, -0.0669, -0.3643, -0.7564,  0.7781, -0.3366],\n",
      "        [ 0.0052, -0.1978, -0.3656, -0.2508,  0.1521,  0.1071],\n",
      "        [ 0.3002, -0.4369,  0.3296,  0.2694,  0.1318,  0.7044],\n",
      "        [-0.4977, -0.0562,  0.0822, -0.5135,  0.2864,  0.1557],\n",
      "        [-0.2423, -0.2762,  0.0833, -0.0817,  0.0571,  0.1861],\n",
      "        [-0.0628, -0.2540,  0.0409, -0.3770,  0.3825, -0.0803],\n",
      "        [-0.2439,  0.2435,  0.4369, -0.3506,  0.6354,  0.5484],\n",
      "        [-0.4202, -0.5976, -0.0340, -0.5976,  0.6007, -0.4556],\n",
      "        [-0.3099, -0.0858, -0.1352, -0.8257,  0.7412, -0.7332],\n",
      "        [-0.1774,  0.1692,  0.2059, -0.8275,  0.7511,  0.1749],\n",
      "        [-0.0523, -0.0602, -0.0387,  0.1284, -0.0143,  0.0696],\n",
      "        [ 0.1186, -0.3444, -0.2416, -0.4395,  0.6609, -0.5192],\n",
      "        [-0.2543,  0.2757, -0.1884, -0.8179,  0.6906, -0.3291],\n",
      "        [ 0.0843,  0.1843,  0.1150,  0.0450,  0.1603,  0.2851]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "expert_mask for expert 1: torch.Size([4, 8]) \n",
      " tensor([[ True, False, False, False,  True,  True,  True,  True],\n",
      "        [ True, False,  True, False, False, False, False,  True],\n",
      "        [ True, False, False,  True, False,  True, False,  True],\n",
      "        [False,  True, False, False, False,  True,  True, False]])\n",
      "flat_mask for expert 1: torch.Size([32]) \n",
      " tensor([ True, False, False, False,  True,  True,  True,  True,  True, False,\n",
      "         True, False, False, False, False,  True,  True, False, False,  True,\n",
      "        False,  True, False,  True, False,  True, False, False, False,  True,\n",
      "         True, False])\n",
      "expert_input for expert 1: torch.Size([15, 6]) \n",
      " tensor([[ 1.2299,  0.5244,  1.4837,  0.1540, -0.2170, -0.8817],\n",
      "        [-1.3578, -0.3675,  1.3089, -0.3086, -0.1582,  1.4662],\n",
      "        [ 0.6899,  1.6619,  0.4237, -1.2295, -0.7147,  0.3079],\n",
      "        [-1.2869,  0.9882,  0.8100, -0.5554,  1.6155,  0.2338],\n",
      "        [ 0.4974,  1.1011,  0.2193, -1.8050,  0.7022,  0.5983],\n",
      "        [-0.5966,  1.4202, -0.7049,  0.2698, -0.3517, -0.8284],\n",
      "        [ 1.2632,  0.5837, -0.5303,  0.2640,  1.3398,  0.6947],\n",
      "        [ 0.7277, -0.2290, -0.3873,  1.1962,  1.0144, -1.3589],\n",
      "        [-0.7201,  0.4455,  0.1452, -0.2854, -0.7594,  0.1062],\n",
      "        [ 0.4958, -1.6009,  0.2999,  1.2335, -2.5198,  1.0030],\n",
      "        [-0.0877, -2.3135, -0.5489, -0.4378, -1.4740,  1.4664],\n",
      "        [ 0.5087,  0.1813,  0.0071,  0.5375,  1.7128, -1.0169],\n",
      "        [ 0.0966, -0.1857, -0.1689, -0.9074, -0.6497,  1.7555],\n",
      "        [ 1.3038, -0.6692,  0.4751, -0.0427, -0.9352,  0.3346],\n",
      "        [ 0.8475, -0.1347, -0.9682, -1.6712, -0.6432, -1.3491]])\n",
      "expert_output for expert 1: torch.Size([15, 6]) \n",
      " tensor([[ 0.6692,  0.3344, -0.4928,  0.1461, -0.1673,  0.6472],\n",
      "        [ 0.0426, -0.7926,  0.1199, -0.3869, -0.6024,  0.5354],\n",
      "        [-0.3720,  0.3729,  0.0091,  0.3356, -0.0257,  0.1675],\n",
      "        [-0.4729, -0.3359,  0.7191, -0.6453, -0.8488,  0.0336],\n",
      "        [-0.4157, -0.1880,  0.3032,  0.4550, -0.0135, -0.4047],\n",
      "        [-0.4768,  0.6013,  0.5951, -0.0585,  0.1385,  0.3892],\n",
      "        [ 0.3096, -0.6651,  0.2916,  0.7138, -0.0919, -0.4959],\n",
      "        [ 0.7001, -0.0301,  0.1838,  0.4264,  0.3375,  0.1481],\n",
      "        [-0.2067,  0.0995,  0.2353,  0.0066,  0.1011,  0.5003],\n",
      "        [ 1.1264, -0.5155, -0.7246,  0.7806,  0.6470,  1.0168],\n",
      "        [ 0.6511, -1.0238, -0.2328,  1.1953,  1.1965,  0.1953],\n",
      "        [ 0.4222, -0.1745,  0.3404,  0.2057, -0.0363, -0.0805],\n",
      "        [-0.0136, -0.7346,  0.0729,  0.7129,  0.2888, -0.0676],\n",
      "        [ 0.8158, -0.2988, -0.5360,  0.8838,  0.5273,  0.3629],\n",
      "        [-0.0682,  0.5637,  0.0823,  1.2029,  1.5554, -0.1219]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "gating_scores for expert 1: torch.Size([15, 1]) \n",
      " tensor([[0.2498],\n",
      "        [0.3782],\n",
      "        [0.6314],\n",
      "        [0.3586],\n",
      "        [0.8138],\n",
      "        [0.3821],\n",
      "        [0.5740],\n",
      "        [0.3890],\n",
      "        [0.7543],\n",
      "        [0.3392],\n",
      "        [0.5171],\n",
      "        [0.3151],\n",
      "        [0.4516],\n",
      "        [0.3511],\n",
      "        [0.1599]], grad_fn=<UnsqueezeBackward0>)\n",
      "weighted_output for expert 1: torch.Size([15, 6]) \n",
      " tensor([[ 0.1672,  0.0835, -0.1231,  0.0365, -0.0418,  0.1616],\n",
      "        [ 0.0161, -0.2998,  0.0454, -0.1463, -0.2278,  0.2025],\n",
      "        [-0.2349,  0.2355,  0.0057,  0.2119, -0.0163,  0.1058],\n",
      "        [-0.1696, -0.1205,  0.2579, -0.2314, -0.3044,  0.0120],\n",
      "        [-0.3383, -0.1530,  0.2467,  0.3703, -0.0110, -0.3293],\n",
      "        [-0.1822,  0.2297,  0.2273, -0.0223,  0.0529,  0.1487],\n",
      "        [ 0.1777, -0.3818,  0.1674,  0.4097, -0.0527, -0.2846],\n",
      "        [ 0.2724, -0.0117,  0.0715,  0.1659,  0.1313,  0.0576],\n",
      "        [-0.1559,  0.0751,  0.1775,  0.0050,  0.0762,  0.3774],\n",
      "        [ 0.3820, -0.1749, -0.2458,  0.2648,  0.2195,  0.3449],\n",
      "        [ 0.3367, -0.5295, -0.1204,  0.6181,  0.6188,  0.1010],\n",
      "        [ 0.1330, -0.0550,  0.1073,  0.0648, -0.0114, -0.0254],\n",
      "        [-0.0062, -0.3318,  0.0329,  0.3220,  0.1304, -0.0305],\n",
      "        [ 0.2864, -0.1049, -0.1882,  0.3103,  0.1851,  0.1274],\n",
      "        [-0.0109,  0.0901,  0.0131,  0.1923,  0.2486, -0.0195]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "expert_mask for expert 2: torch.Size([4, 8]) \n",
      " tensor([[ True,  True,  True, False,  True,  True, False,  True],\n",
      "        [ True,  True, False,  True,  True,  True,  True, False],\n",
      "        [ True,  True, False,  True,  True, False,  True,  True],\n",
      "        [ True, False,  True,  True,  True,  True,  True, False]])\n",
      "flat_mask for expert 2: torch.Size([32]) \n",
      " tensor([ True,  True,  True, False,  True,  True, False,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True, False,  True,  True, False,  True,\n",
      "         True, False,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True, False])\n",
      "expert_input for expert 2: torch.Size([24, 6]) \n",
      " tensor([[ 1.2299e+00,  5.2441e-01,  1.4837e+00,  1.5396e-01, -2.1697e-01,\n",
      "         -8.8170e-01],\n",
      "        [-7.1175e-01, -3.1869e-01,  4.7824e-02, -5.2963e-01, -1.4327e+00,\n",
      "          2.3175e-01],\n",
      "        [-2.8373e-01,  1.0245e+00,  9.2586e-01,  1.9426e+00, -3.8501e-01,\n",
      "          6.0114e-01],\n",
      "        [-1.3578e+00, -3.6746e-01,  1.3089e+00, -3.0863e-01, -1.5820e-01,\n",
      "          1.4662e+00],\n",
      "        [ 6.8992e-01,  1.6619e+00,  4.2371e-01, -1.2295e+00, -7.1474e-01,\n",
      "          3.0786e-01],\n",
      "        [ 4.9741e-01,  1.1011e+00,  2.1932e-01, -1.8050e+00,  7.0224e-01,\n",
      "          5.9832e-01],\n",
      "        [-5.9661e-01,  1.4202e+00, -7.0488e-01,  2.6981e-01, -3.5171e-01,\n",
      "         -8.2836e-01],\n",
      "        [-6.4758e-01, -8.0608e-01, -2.0871e-01,  2.6211e-01, -4.5044e-01,\n",
      "         -3.9843e-01],\n",
      "        [ 7.6762e-01,  9.4395e-03,  3.9564e-01,  3.2025e-01, -2.8774e-01,\n",
      "          2.5670e-01],\n",
      "        [-6.7316e-02, -4.8072e-01, -2.0145e-03, -2.9684e-01,  4.9095e-01,\n",
      "         -3.7736e-01],\n",
      "        [ 1.0783e+00, -1.2894e+00,  2.5983e-01,  1.1599e+00,  8.2941e-01,\n",
      "          1.7601e-02],\n",
      "        [-1.1930e+00, -7.3533e-01, -5.9583e-01, -6.1060e-01, -1.4336e-01,\n",
      "          4.0583e-01],\n",
      "        [-7.2012e-01,  4.4555e-01,  1.4524e-01, -2.8537e-01, -7.5935e-01,\n",
      "          1.0620e-01],\n",
      "        [-7.1099e-01,  3.5681e-01,  6.9835e-01,  2.3638e+00,  5.8684e-01,\n",
      "          1.2924e+00],\n",
      "        [ 4.9582e-01, -1.6009e+00,  2.9994e-01,  1.2335e+00, -2.5198e+00,\n",
      "          1.0030e+00],\n",
      "        [-5.5582e-01, -1.7669e+00, -5.2814e-01, -1.2567e+00, -1.8438e-01,\n",
      "         -3.8586e-02],\n",
      "        [-2.5843e+00, -4.8744e-01,  7.8507e-03,  3.6422e-01, -7.7910e-01,\n",
      "          3.4041e-01],\n",
      "        [ 5.0866e-01,  1.8129e-01,  7.0565e-03,  5.3748e-01,  1.7128e+00,\n",
      "         -1.0169e+00],\n",
      "        [ 1.0021e+00,  5.1247e-01,  1.6864e+00, -6.9051e-01,  5.6580e-01,\n",
      "          1.4997e-01],\n",
      "        [ 7.1586e-01, -3.8600e-01,  7.5606e-01,  4.5368e-01,  5.5452e-01,\n",
      "          6.4531e-01],\n",
      "        [-1.2511e+00, -6.2662e-01, -1.6361e-01,  5.7103e-01, -2.5860e+00,\n",
      "          1.1485e+00],\n",
      "        [ 6.8176e-01,  1.6960e+00,  2.9258e-01, -1.0163e+00,  7.7617e-01,\n",
      "         -1.7823e+00],\n",
      "        [ 1.3038e+00, -6.6918e-01,  4.7511e-01, -4.2731e-02, -9.3523e-01,\n",
      "          3.3459e-01],\n",
      "        [ 8.4746e-01, -1.3469e-01, -9.6825e-01, -1.6712e+00, -6.4324e-01,\n",
      "         -1.3491e+00]])\n",
      "expert_output for expert 2: torch.Size([24, 6]) \n",
      " tensor([[-0.1266, -0.5206,  0.5715, -0.7246,  0.6879, -0.0073],\n",
      "        [ 0.9075,  0.3761,  0.3582,  0.1900, -0.4575,  0.4253],\n",
      "        [ 0.8208, -0.6921,  0.2302, -0.6353,  1.4072, -0.8671],\n",
      "        [ 1.4228, -0.3902,  0.3951,  0.7101,  0.7546,  0.0877],\n",
      "        [-0.1279,  0.1726,  0.9319,  0.2755,  0.0990,  0.3493],\n",
      "        [-0.2931, -0.0122,  0.5768,  1.0158,  0.3822,  0.4701],\n",
      "        [-0.2248,  0.2030,  0.4780,  0.0768,  0.0469, -0.3133],\n",
      "        [ 0.6326,  0.0634, -0.1337,  0.0172, -0.0765,  0.1016],\n",
      "        [ 0.3901, -0.2533, -0.0304, -0.3009,  0.5563,  0.0335],\n",
      "        [ 0.1436, -0.1429, -0.0852,  0.3094,  0.2597,  0.1758],\n",
      "        [ 0.4068, -0.6748, -0.8739, -0.4896,  1.0338, -0.1502],\n",
      "        [ 0.7617,  0.2664, -0.1095,  0.8276, -0.2086,  0.3362],\n",
      "        [ 0.5472,  0.1340,  0.4929,  0.2626,  0.0113,  0.0973],\n",
      "        [ 1.1281, -0.9741, -0.3334, -0.2588,  1.9172, -1.0580],\n",
      "        [ 1.8072,  0.1230, -0.4867, -1.0713, -0.1313,  0.2722],\n",
      "        [ 0.6617,  0.3839, -0.3135,  0.7530, -0.6472,  0.8335],\n",
      "        [ 1.3783,  0.1085,  0.2591,  0.5836, -0.0119, -0.1482],\n",
      "        [-0.5675, -0.6214, -0.2472,  0.0310,  1.0316, -0.3678],\n",
      "        [ 0.0133, -0.6396,  0.5826,  0.0714,  0.9960,  0.1840],\n",
      "        [ 0.5147, -0.6155, -0.2617, -0.0933,  1.0813, -0.0675],\n",
      "        [ 1.8442,  0.4800,  0.0952, -0.1916, -0.4637,  0.1857],\n",
      "        [-1.2312, -0.1143,  0.9025,  0.1884,  0.2467,  0.0535],\n",
      "        [ 0.5952, -0.0641, -0.1362, -0.5060,  0.1445,  0.4332],\n",
      "        [-0.5725,  0.7194,  0.2177,  0.2260, -1.0888,  0.8880]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "gating_scores for expert 2: torch.Size([24, 1]) \n",
      " tensor([[0.7502],\n",
      "        [0.2251],\n",
      "        [0.3653],\n",
      "        [0.6218],\n",
      "        [0.3686],\n",
      "        [0.1862],\n",
      "        [0.6179],\n",
      "        [0.3718],\n",
      "        [0.4473],\n",
      "        [0.5183],\n",
      "        [0.2734],\n",
      "        [0.5908],\n",
      "        [0.2457],\n",
      "        [0.1928],\n",
      "        [0.6608],\n",
      "        [0.2778],\n",
      "        [0.3895],\n",
      "        [0.6849],\n",
      "        [0.8154],\n",
      "        [0.6195],\n",
      "        [0.5805],\n",
      "        [0.6423],\n",
      "        [0.6489],\n",
      "        [0.8401]], grad_fn=<UnsqueezeBackward0>)\n",
      "weighted_output for expert 2: torch.Size([24, 6]) \n",
      " tensor([[-0.0949, -0.3906,  0.4288, -0.5436,  0.5161, -0.0055],\n",
      "        [ 0.2043,  0.0847,  0.0807,  0.0428, -0.1030,  0.0958],\n",
      "        [ 0.2999, -0.2529,  0.0841, -0.2321,  0.5141, -0.3168],\n",
      "        [ 0.8847, -0.2426,  0.2457,  0.4415,  0.4692,  0.0545],\n",
      "        [-0.0472,  0.0636,  0.3435,  0.1015,  0.0365,  0.1287],\n",
      "        [-0.0546, -0.0023,  0.1074,  0.1892,  0.0712,  0.0875],\n",
      "        [-0.1389,  0.1254,  0.2954,  0.0475,  0.0290, -0.1936],\n",
      "        [ 0.2352,  0.0236, -0.0497,  0.0064, -0.0284,  0.0378],\n",
      "        [ 0.1745, -0.1133, -0.0136, -0.1346,  0.2488,  0.0150],\n",
      "        [ 0.0744, -0.0740, -0.0442,  0.1603,  0.1346,  0.0911],\n",
      "        [ 0.1112, -0.1845, -0.2390, -0.1339,  0.2827, -0.0411],\n",
      "        [ 0.4500,  0.1574, -0.0647,  0.4890, -0.1232,  0.1986],\n",
      "        [ 0.1344,  0.0329,  0.1211,  0.0645,  0.0028,  0.0239],\n",
      "        [ 0.2175, -0.1878, -0.0643, -0.0499,  0.3697, -0.2040],\n",
      "        [ 1.1942,  0.0813, -0.3216, -0.7079, -0.0868,  0.1799],\n",
      "        [ 0.1838,  0.1066, -0.0871,  0.2092, -0.1798,  0.2315],\n",
      "        [ 0.5369,  0.0423,  0.1009,  0.2273, -0.0047, -0.0577],\n",
      "        [-0.3887, -0.4256, -0.1693,  0.0212,  0.7065, -0.2519],\n",
      "        [ 0.0108, -0.5215,  0.4751,  0.0582,  0.8121,  0.1500],\n",
      "        [ 0.3188, -0.3813, -0.1621, -0.0578,  0.6699, -0.0418],\n",
      "        [ 1.0706,  0.2787,  0.0553, -0.1112, -0.2692,  0.1078],\n",
      "        [-0.7908, -0.0734,  0.5797,  0.1210,  0.1585,  0.0344],\n",
      "        [ 0.3862, -0.0416, -0.0884, -0.3283,  0.0938,  0.2811],\n",
      "        [-0.4809,  0.6044,  0.1829,  0.1898, -0.9148,  0.7461]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "expert_mask for expert 3: torch.Size([4, 8]) \n",
      " tensor([[False, False,  True,  True, False, False, False, False],\n",
      "        [False, False,  True,  True, False,  True, False,  True],\n",
      "        [False,  True,  True, False, False, False, False, False],\n",
      "        [False, False,  True, False,  True, False, False,  True]])\n",
      "flat_mask for expert 3: torch.Size([32]) \n",
      " tensor([False, False,  True,  True, False, False, False, False, False, False,\n",
      "         True,  True, False,  True, False,  True, False,  True,  True, False,\n",
      "        False, False, False, False, False, False,  True, False,  True, False,\n",
      "        False,  True])\n",
      "expert_input for expert 3: torch.Size([11, 6]) \n",
      " tensor([[-0.2837,  1.0245,  0.9259,  1.9426, -0.3850,  0.6011],\n",
      "        [ 0.6188,  2.2128, -0.9450,  0.3971, -1.3050,  0.4143],\n",
      "        [ 1.2632,  0.5837, -0.5303,  0.2640,  1.3398,  0.6947],\n",
      "        [ 0.7676,  0.0094,  0.3956,  0.3202, -0.2877,  0.2567],\n",
      "        [ 1.0783, -1.2894,  0.2598,  1.1599,  0.8294,  0.0176],\n",
      "        [ 0.7277, -0.2290, -0.3873,  1.1962,  1.0144, -1.3589],\n",
      "        [-0.7110,  0.3568,  0.6984,  2.3638,  0.5868,  1.2924],\n",
      "        [-1.4764, -0.3692,  1.3290,  0.1512,  0.3008,  0.3918],\n",
      "        [ 0.7159, -0.3860,  0.7561,  0.4537,  0.5545,  0.6453],\n",
      "        [ 0.6818,  1.6960,  0.2926, -1.0163,  0.7762, -1.7823],\n",
      "        [-0.0199,  1.0562,  1.9374,  0.9202,  0.5043,  1.5896]])\n",
      "expert_output for expert 3: torch.Size([11, 6]) \n",
      " tensor([[ 0.9776, -0.0171,  0.1840,  0.0306, -0.6041,  0.2838],\n",
      "        [ 0.6061, -0.5465, -1.3647,  0.6799,  0.1333,  0.1377],\n",
      "        [-0.1546,  0.0403, -0.3842, -0.7047,  0.1755, -0.6761],\n",
      "        [ 0.3917,  0.0427, -0.2174, -0.3912,  0.0472,  0.0811],\n",
      "        [ 0.4580,  0.1824,  0.3382, -1.1402,  0.3376, -0.1325],\n",
      "        [ 0.6803,  0.6126,  0.2964, -1.3074,  0.7645, -0.0835],\n",
      "        [ 0.6740,  0.0095,  0.5751,  0.0121, -0.7667, -0.1029],\n",
      "        [ 0.0355,  0.9133,  0.9743, -0.1235, -0.6399,  0.0206],\n",
      "        [ 0.1568,  0.2109,  0.2000, -0.6925, -0.1736, -0.1772],\n",
      "        [ 0.0762,  1.2026, -0.0389, -1.2240,  0.4836, -0.1411],\n",
      "        [ 0.2359,  0.2254,  0.4575, -0.2467, -1.1907, -0.1386]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "gating_scores for expert 3: torch.Size([11, 1]) \n",
      " tensor([[0.6347],\n",
      "        [0.5325],\n",
      "        [0.4260],\n",
      "        [0.5527],\n",
      "        [0.7266],\n",
      "        [0.6110],\n",
      "        [0.8072],\n",
      "        [0.0620],\n",
      "        [0.3805],\n",
      "        [0.3577],\n",
      "        [0.6126]], grad_fn=<UnsqueezeBackward0>)\n",
      "weighted_output for expert 3: torch.Size([11, 6]) \n",
      " tensor([[ 0.6205, -0.0109,  0.1168,  0.0194, -0.3834,  0.1801],\n",
      "        [ 0.3228, -0.2910, -0.7267,  0.3620,  0.0710,  0.0733],\n",
      "        [-0.0659,  0.0172, -0.1637, -0.3002,  0.0748, -0.2880],\n",
      "        [ 0.2165,  0.0236, -0.1201, -0.2162,  0.0261,  0.0448],\n",
      "        [ 0.3328,  0.1325,  0.2457, -0.8284,  0.2453, -0.0963],\n",
      "        [ 0.4157,  0.3743,  0.1811, -0.7988,  0.4671, -0.0510],\n",
      "        [ 0.5440,  0.0077,  0.4642,  0.0097, -0.6189, -0.0831],\n",
      "        [ 0.0022,  0.0566,  0.0604, -0.0077, -0.0397,  0.0013],\n",
      "        [ 0.0597,  0.0803,  0.0761, -0.2635, -0.0660, -0.0674],\n",
      "        [ 0.0272,  0.4301, -0.0139, -0.4378,  0.1730, -0.0505],\n",
      "        [ 0.1445,  0.1381,  0.2802, -0.1511, -0.7294, -0.0849]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "final_output after processing: torch.Size([4, 8, 6]) \n",
      " tensor([[[ 0.0722, -0.3070,  0.3057, -0.5071,  0.4743,  0.1561],\n",
      "         [-0.2041,  0.0178, -0.2837, -0.7137,  0.6751, -0.2408],\n",
      "         [ 0.9203, -0.2637,  0.2009, -0.2127,  0.1307, -0.1367],\n",
      "         [ 0.3279, -0.4888, -1.0922,  0.1112,  0.2230,  0.1805],\n",
      "         [ 0.9008, -0.5424,  0.2910,  0.2952,  0.2414,  0.2570],\n",
      "         [-0.2821,  0.2991,  0.3492,  0.3135,  0.0202,  0.2345],\n",
      "         [ 0.1306, -0.5574,  0.5875,  0.0380, -0.1726,  0.7165],\n",
      "         [-0.3929, -0.1553,  0.3541,  0.5594,  0.0602, -0.2418]],\n",
      "\n",
      "        [[-0.3211,  0.3551,  0.5227,  0.0251,  0.0819, -0.0449],\n",
      "         [-0.2625, -0.0326,  0.0325, -0.5071,  0.2579,  0.1935],\n",
      "         [ 0.1118, -0.3646,  0.0037,  0.1095,  0.0221, -0.5727],\n",
      "         [ 0.3910, -0.0897, -0.1337, -0.3508,  0.2749,  0.0598],\n",
      "         [-0.1678, -0.3502,  0.0392,  0.0786,  0.1917,  0.2772],\n",
      "         [ 0.4440, -0.0520,  0.0068, -0.9623,  0.5280, -0.1373],\n",
      "         [ 0.3872, -0.0966, -0.0238,  0.1120,  0.2593,  0.1183],\n",
      "         [ 0.6880,  0.3625,  0.2526, -0.6329,  0.5984,  0.0066]],\n",
      "\n",
      "        [[-0.0215,  0.1080,  0.2986,  0.0695,  0.0790,  0.4013],\n",
      "         [ 0.7615, -0.1801,  0.3999, -0.0402, -0.2492, -0.2871],\n",
      "         [-0.2417,  0.3001,  0.4973, -0.3582,  0.5957,  0.5496],\n",
      "         [ 1.5763, -0.0936, -0.5674, -0.4432,  0.1327,  0.5247],\n",
      "         [-0.2364, -0.4909, -0.1211, -0.3884,  0.4209, -0.2240],\n",
      "         [ 0.0268, -0.6153, -0.2556, -0.2075,  1.3599, -0.6322],\n",
      "         [ 0.3595,  0.2114,  0.3069, -0.6001,  0.7465,  0.1172],\n",
      "         [-0.2556, -0.4806, -0.0621,  0.0860,  0.6951, -0.2773]],\n",
      "\n",
      "        [[-0.0415, -0.5816,  0.4364,  0.1866,  0.7979,  0.2197],\n",
      "         [ 0.1125, -0.6762, -0.2087, -0.1175,  0.7914, -0.5497],\n",
      "         [ 0.3785, -0.3010, -0.0860, -0.3213,  0.6038, -0.1092],\n",
      "         [ 0.8163,  0.5543, -0.1331, -0.9292,  0.4214, -0.2213],\n",
      "         [-0.7636,  0.3567,  0.5658, -0.3168,  0.3314, -0.0161],\n",
      "         [ 0.6727, -0.1465, -0.2766, -0.0180,  0.2789,  0.4085],\n",
      "         [-0.4918,  0.6945,  0.1960,  0.3821, -0.6661,  0.7266],\n",
      "         [ 0.2288,  0.3223,  0.3953, -0.1061, -0.5691,  0.2002]]],\n",
      "       grad_fn=<IndexPutBackward0>)\n",
      "Shape of the final output: torch.Size([4, 8, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embd = 6\n",
    "dropout=0.1\n",
    "\n",
    "mh_output = torch.randn(4, 8, n_embd)  # Example multi-head attention output\n",
    "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
    "final_output = sparse_moe(mh_output)\n",
    "print(\"Shape of the final output:\", final_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        queries = self.W_query(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = self.W_value(x).view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        context_vec = context_vec.transpose(1, 2).contiguous().view(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOEBLOCK(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head, num_experts, top_k , context_length):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_embed,n_embed,context_length,dropout=None,num_heads=n_head)\n",
    "        self.smoe = SparseMoE(n_embed, num_experts, top_k)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        shortcut = x \n",
    "        x = self.sa(self.ln1(x))\n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        x = self.smoe(self.ln2(x))\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoELM(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(cfg[\"vocab_size\"],cfg[\"n_embed\"])\n",
    "        self.positional_embedding = nn.Embedding(cfg[\"vocab_size\"],cfg[\"n_embed\"])\n",
    "        self.blocks = nn.Sequential(*[MOEBLOCK(cfg[\"n_embed\"],cfg[\"n_head\"],cfg[\"num_experts\"],cfg[\"top_k\"],cfg[\"context_length\"])for _ in range(cfg[\"n_layers\"])])\n",
    "        self.finalNorm = nn.LayerNorm(cfg[\"n_embed\"])\n",
    "        self.lm_head = nn.Linear(cfg[\"n_embed\"],cfg[\"vocab_size\"])\n",
    "\n",
    "    def forward(self,idx):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.embeddings(idx)\n",
    "        pos_emb = self.positional_embedding(idx)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.finalNorm(x)\n",
    "        logits = self.lm_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
